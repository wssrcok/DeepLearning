{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "c8Pqt5O58m-T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "bX7yVwIl8sQI"
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "cViWqKB28xA5"
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache\n",
    "  \n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "\n",
    "    Z_exp = np.exp(Z);\n",
    "    den = np.sum(Z_exp, axis = 0);\n",
    "    A = Z_exp / den;\n",
    "\n",
    "    assert(A.shape == Z.shape)\n",
    "\n",
    "    cache = Z \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "  \n",
    "def softmax_backward(Y, Y_hat):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "    Arguments:\n",
    "    \n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    dZ = Y_hat - Y\n",
    "    \n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "29fGpjay83hR"
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    Z = np.dot(W,A)+b\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Svtr1Aas9Aqs"
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        ### END CODE HERE ###\n",
    "    elif activation == \"softmax\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "        \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "nq6F-n5a9ESE"
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], \n",
    "                                             parameters['b' + str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], \n",
    "                                          parameters['b' + str(L)], \"softmax\")\n",
    "    caches.append(cache)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    #assert(AL.shape == (10,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-F4EzHNSH2ur"
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y, cost_function = 'softmax_cross_entropy'):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (10, number of examples)\n",
    "    Y -- true \"label\" vector (for example: [1,0,0,...,0] as 0\n",
    "         [0,1,0,...,0] as 1), shape (classes, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    if cost_function == 'softmax_cross_entropy':\n",
    "        cost = (-1/m) * np.sum(Y*np.log(AL))\n",
    "    elif cost_function == 'sigmoid_cross_entropy':\n",
    "        cost = -1/m*np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "iv2IsSaEH_Np"
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    dW = 1/m*np.dot(dZ,A_prev.T)\n",
    "    db = 1/m*np.sum(dZ,axis=1,keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gSWAbuSbIAGd"
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, Y, Y_hat, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "    elif activation == \"softmax\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = softmax_backward(Y, Y_hat)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "R9Vzq9C0ICTt"
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    current_cache = caches[L-1]\n",
    "    #grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, 'sigmoid')\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, Y, AL, 'softmax')\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, Y, AL, 'relu')\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "eqcK9yeUIFZ1"
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] -= learning_rate * grads['dW'+str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] -= learning_rate * grads['db'+str(l+1)]\n",
    "    ### END CODE HERE ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "C0veWoz3P5XJ"
   },
   "outputs": [],
   "source": [
    "def one_hot_label(classes, label):\n",
    "    \"\"\"\n",
    "    reshape label to Sam prefered shape for mnist\n",
    "\n",
    "    Arguments:\n",
    "    label -- input label with shape (m,)\n",
    "\n",
    "    Returns:\n",
    "    new_label -- output label with shape (10, 1, m)\n",
    "    \"\"\"\n",
    "    m = label.shape[0]\n",
    "    new_label = np.zeros((classes, m))\n",
    "    for i in range(m):\n",
    "        clas = label[i]\n",
    "        new_label[clas,i] = 1\n",
    "    return new_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12288, 1080)\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels, eval_data, eval_labels, classes = load_dataset()\n",
    "classes = 6\n",
    "train_labels = np.squeeze(train_labels)\n",
    "train_labels = one_hot_label(classes, train_labels)\n",
    "eval_labels = np.squeeze(eval_labels)\n",
    "eval_labels = one_hot_label(classes, eval_labels)\n",
    "# Flatten the training and test images\n",
    "train_data = train_data.reshape(train_data.shape[0], -1).T\n",
    "eval_data = eval_data.reshape(eval_data.shape[0], -1).T\n",
    "# Normalize image vectors\n",
    "train_data = train_data/255.\n",
    "eval_data = eval_data/255.\n",
    "features = train_data.shape[0]\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 713
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5471,
     "status": "ok",
     "timestamp": 1524423903496,
     "user": {
      "displayName": "Sam Shen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "100562431021972138930"
     },
     "user_tz": 420
    },
    "id": "iUnl80CzJRW3",
    "outputId": "a53edd3c-90b7-4072-be10-8deea3b77f95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n",
      "(784, 55000)\n"
     ]
    }
   ],
   "source": [
    "# Load training and eval data\n",
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "train_data = mnist.train.images.T # Returns np.array\n",
    "classes = 10\n",
    "train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "train_labels = one_hot_label(classes, train_labels)\n",
    "eval_data = mnist.test.images.T # Returns np.array\n",
    "eval_labels_old = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "eval_labels = one_hot_label(classes, eval_labels_old)\n",
    "features = train_data.shape[0]\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1524423903884,
     "user": {
      "displayName": "Sam Shen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "100562431021972138930"
     },
     "user_tz": 420
    },
    "id": "sfod1faNVngV",
    "outputId": "e7a6c559-a8c9-47f3-a6e7-157625017986"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 55000)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Vg_o4v6tKHMF"
   },
   "outputs": [],
   "source": [
    "layers_dims = [features, 25, 12, classes] #  2-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KYtvwudOKHk0"
   },
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, parameters = {}, batch_size = 64, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization. (≈ 1 line of code)\n",
    "    if len(parameters) == 0:\n",
    "        parameters = initialize_parameters_deep(layers_dims)\n",
    "    m = X.shape[1]\n",
    "    num_batchs = m // batch_size\n",
    "    \n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "        for j in range(num_batchs):\n",
    "            # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "            # AL is the output and caches contains Z, A, W, b for each layer\n",
    "            AL, caches = L_model_forward(X[:, j*batch_size:(j+1)*batch_size], parameters)\n",
    "            # Compute cost.\n",
    "            cost = compute_cost(AL,Y[:, j*batch_size:(j+1)*batch_size])\n",
    "            # Backward propagation.\n",
    "            grads = L_model_backward(AL, Y[:, j*batch_size:(j+1)*batch_size], caches)\n",
    "            # Update parameters.\n",
    "            parameters = update_parameters(parameters, grads, learning_rate)\n",
    "            # Print the cost every 30 training example\n",
    "#             if print_cost and j % 10 == 0:\n",
    "#                 print (\"Cost after iteration %i, batch %i: %f\" %(i, j, cost))\n",
    "            if print_cost and j % 10 == 0:\n",
    "                costs.append(cost)\n",
    "        if print_cost and i % 1 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 988
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 162839,
     "status": "ok",
     "timestamp": 1524424075781,
     "user": {
      "displayName": "Sam Shen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "100562431021972138930"
     },
     "user_tz": 420
    },
    "id": "AZlK7EE4crMa",
    "outputId": "c6a279bb-a9e3-4e9e-afa6-fed4d8bfd9ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0, batch 0: 1.791724\n",
      "Cost after iteration 0, batch 10: 1.791726\n",
      "Cost after iteration 0, batch 20: 1.791730\n",
      "Cost after iteration 0, batch 30: 1.791766\n",
      "Cost after iteration 0: 1.791866\n",
      "Cost after iteration 1, batch 0: 1.791725\n",
      "Cost after iteration 1, batch 10: 1.791726\n",
      "Cost after iteration 1, batch 20: 1.791730\n",
      "Cost after iteration 1, batch 30: 1.791767\n",
      "Cost after iteration 1: 1.791867\n",
      "Cost after iteration 2, batch 0: 1.791725\n",
      "Cost after iteration 2, batch 10: 1.791727\n",
      "Cost after iteration 2, batch 20: 1.791730\n",
      "Cost after iteration 2, batch 30: 1.791768\n",
      "Cost after iteration 2: 1.791869\n",
      "Cost after iteration 3, batch 0: 1.791726\n",
      "Cost after iteration 3, batch 10: 1.791727\n",
      "Cost after iteration 3, batch 20: 1.791731\n",
      "Cost after iteration 3, batch 30: 1.791769\n",
      "Cost after iteration 3: 1.791870\n",
      "Cost after iteration 4, batch 0: 1.791726\n",
      "Cost after iteration 4, batch 10: 1.791728\n",
      "Cost after iteration 4, batch 20: 1.791731\n",
      "Cost after iteration 4, batch 30: 1.791769\n",
      "Cost after iteration 4: 1.791871\n",
      "Cost after iteration 5, batch 0: 1.791726\n",
      "Cost after iteration 5, batch 10: 1.791728\n",
      "Cost after iteration 5, batch 20: 1.791731\n",
      "Cost after iteration 5, batch 30: 1.791770\n",
      "Cost after iteration 5: 1.791873\n",
      "Cost after iteration 6, batch 0: 1.791727\n",
      "Cost after iteration 6, batch 10: 1.791729\n",
      "Cost after iteration 6, batch 20: 1.791731\n",
      "Cost after iteration 6, batch 30: 1.791771\n",
      "Cost after iteration 6: 1.791874\n",
      "Cost after iteration 7, batch 0: 1.791727\n",
      "Cost after iteration 7, batch 10: 1.791729\n",
      "Cost after iteration 7, batch 20: 1.791732\n",
      "Cost after iteration 7, batch 30: 1.791772\n",
      "Cost after iteration 7: 1.791875\n",
      "Cost after iteration 8, batch 0: 1.791728\n",
      "Cost after iteration 8, batch 10: 1.791730\n",
      "Cost after iteration 8, batch 20: 1.791732\n",
      "Cost after iteration 8, batch 30: 1.791773\n",
      "Cost after iteration 8: 1.791877\n",
      "Cost after iteration 9, batch 0: 1.791728\n",
      "Cost after iteration 9, batch 10: 1.791730\n",
      "Cost after iteration 9, batch 20: 1.791732\n",
      "Cost after iteration 9, batch 30: 1.791774\n",
      "Cost after iteration 9: 1.791878\n",
      "Cost after iteration 10, batch 0: 1.791728\n",
      "Cost after iteration 10, batch 10: 1.791731\n",
      "Cost after iteration 10, batch 20: 1.791732\n",
      "Cost after iteration 10, batch 30: 1.791775\n",
      "Cost after iteration 10: 1.791879\n",
      "Cost after iteration 11, batch 0: 1.791729\n",
      "Cost after iteration 11, batch 10: 1.791732\n",
      "Cost after iteration 11, batch 20: 1.791733\n",
      "Cost after iteration 11, batch 30: 1.791776\n",
      "Cost after iteration 11: 1.791881\n",
      "Cost after iteration 12, batch 0: 1.791729\n",
      "Cost after iteration 12, batch 10: 1.791732\n",
      "Cost after iteration 12, batch 20: 1.791733\n",
      "Cost after iteration 12, batch 30: 1.791777\n",
      "Cost after iteration 12: 1.791882\n",
      "Cost after iteration 13, batch 0: 1.791729\n",
      "Cost after iteration 13, batch 10: 1.791733\n",
      "Cost after iteration 13, batch 20: 1.791733\n",
      "Cost after iteration 13, batch 30: 1.791777\n",
      "Cost after iteration 13: 1.791884\n",
      "Cost after iteration 14, batch 0: 1.791730\n",
      "Cost after iteration 14, batch 10: 1.791733\n",
      "Cost after iteration 14, batch 20: 1.791734\n",
      "Cost after iteration 14, batch 30: 1.791778\n",
      "Cost after iteration 14: 1.791885\n",
      "Cost after iteration 15, batch 0: 1.791730\n",
      "Cost after iteration 15, batch 10: 1.791734\n",
      "Cost after iteration 15, batch 20: 1.791734\n",
      "Cost after iteration 15, batch 30: 1.791779\n",
      "Cost after iteration 15: 1.791886\n",
      "Cost after iteration 16, batch 0: 1.791731\n",
      "Cost after iteration 16, batch 10: 1.791734\n",
      "Cost after iteration 16, batch 20: 1.791734\n",
      "Cost after iteration 16, batch 30: 1.791780\n",
      "Cost after iteration 16: 1.791888\n",
      "Cost after iteration 17, batch 0: 1.791731\n",
      "Cost after iteration 17, batch 10: 1.791735\n",
      "Cost after iteration 17, batch 20: 1.791734\n",
      "Cost after iteration 17, batch 30: 1.791781\n",
      "Cost after iteration 17: 1.791889\n",
      "Cost after iteration 18, batch 0: 1.791731\n",
      "Cost after iteration 18, batch 10: 1.791735\n",
      "Cost after iteration 18, batch 20: 1.791735\n",
      "Cost after iteration 18, batch 30: 1.791782\n",
      "Cost after iteration 18: 1.791890\n",
      "Cost after iteration 19, batch 0: 1.791732\n",
      "Cost after iteration 19, batch 10: 1.791736\n",
      "Cost after iteration 19, batch 20: 1.791735\n",
      "Cost after iteration 19, batch 30: 1.791783\n",
      "Cost after iteration 19: 1.791892\n",
      "Cost after iteration 20, batch 0: 1.791732\n",
      "Cost after iteration 20, batch 10: 1.791737\n",
      "Cost after iteration 20, batch 20: 1.791735\n",
      "Cost after iteration 20, batch 30: 1.791784\n",
      "Cost after iteration 20: 1.791893\n",
      "Cost after iteration 21, batch 0: 1.791733\n",
      "Cost after iteration 21, batch 10: 1.791737\n",
      "Cost after iteration 21, batch 20: 1.791735\n",
      "Cost after iteration 21, batch 30: 1.791785\n",
      "Cost after iteration 21: 1.791894\n",
      "Cost after iteration 22, batch 0: 1.791733\n",
      "Cost after iteration 22, batch 10: 1.791738\n",
      "Cost after iteration 22, batch 20: 1.791736\n",
      "Cost after iteration 22, batch 30: 1.791785\n",
      "Cost after iteration 22: 1.791896\n",
      "Cost after iteration 23, batch 0: 1.791733\n",
      "Cost after iteration 23, batch 10: 1.791738\n",
      "Cost after iteration 23, batch 20: 1.791736\n",
      "Cost after iteration 23, batch 30: 1.791786\n",
      "Cost after iteration 23: 1.791897\n",
      "Cost after iteration 24, batch 0: 1.791734\n",
      "Cost after iteration 24, batch 10: 1.791739\n",
      "Cost after iteration 24, batch 20: 1.791736\n",
      "Cost after iteration 24, batch 30: 1.791787\n",
      "Cost after iteration 24: 1.791898\n",
      "Cost after iteration 25, batch 0: 1.791734\n",
      "Cost after iteration 25, batch 10: 1.791739\n",
      "Cost after iteration 25, batch 20: 1.791737\n",
      "Cost after iteration 25, batch 30: 1.791788\n",
      "Cost after iteration 25: 1.791900\n",
      "Cost after iteration 26, batch 0: 1.791734\n",
      "Cost after iteration 26, batch 10: 1.791740\n",
      "Cost after iteration 26, batch 20: 1.791737\n",
      "Cost after iteration 26, batch 30: 1.791789\n",
      "Cost after iteration 26: 1.791901\n",
      "Cost after iteration 27, batch 0: 1.791735\n",
      "Cost after iteration 27, batch 10: 1.791740\n",
      "Cost after iteration 27, batch 20: 1.791737\n",
      "Cost after iteration 27, batch 30: 1.791790\n",
      "Cost after iteration 27: 1.791903\n",
      "Cost after iteration 28, batch 0: 1.791735\n",
      "Cost after iteration 28, batch 10: 1.791741\n",
      "Cost after iteration 28, batch 20: 1.791737\n",
      "Cost after iteration 28, batch 30: 1.791791\n",
      "Cost after iteration 28: 1.791904\n",
      "Cost after iteration 29, batch 0: 1.791736\n",
      "Cost after iteration 29, batch 10: 1.791741\n",
      "Cost after iteration 29, batch 20: 1.791738\n",
      "Cost after iteration 29, batch 30: 1.791792\n",
      "Cost after iteration 29: 1.791905\n",
      "Cost after iteration 30, batch 0: 1.791736\n",
      "Cost after iteration 30, batch 10: 1.791742\n",
      "Cost after iteration 30, batch 20: 1.791738\n",
      "Cost after iteration 30, batch 30: 1.791792\n",
      "Cost after iteration 30: 1.791907\n",
      "Cost after iteration 31, batch 0: 1.791736\n",
      "Cost after iteration 31, batch 10: 1.791742\n",
      "Cost after iteration 31, batch 20: 1.791738\n",
      "Cost after iteration 31, batch 30: 1.791793\n",
      "Cost after iteration 31: 1.791908\n",
      "Cost after iteration 32, batch 0: 1.791737\n",
      "Cost after iteration 32, batch 10: 1.791743\n",
      "Cost after iteration 32, batch 20: 1.791738\n",
      "Cost after iteration 32, batch 30: 1.791794\n",
      "Cost after iteration 32: 1.791909\n",
      "Cost after iteration 33, batch 0: 1.791737\n",
      "Cost after iteration 33, batch 10: 1.791744\n",
      "Cost after iteration 33, batch 20: 1.791739\n",
      "Cost after iteration 33, batch 30: 1.791795\n",
      "Cost after iteration 33: 1.791911\n",
      "Cost after iteration 34, batch 0: 1.791737\n",
      "Cost after iteration 34, batch 10: 1.791744\n",
      "Cost after iteration 34, batch 20: 1.791739\n",
      "Cost after iteration 34, batch 30: 1.791796\n",
      "Cost after iteration 34: 1.791912\n",
      "Cost after iteration 35, batch 0: 1.791738\n",
      "Cost after iteration 35, batch 10: 1.791745\n",
      "Cost after iteration 35, batch 20: 1.791739\n",
      "Cost after iteration 35, batch 30: 1.791797\n",
      "Cost after iteration 35: 1.791913\n",
      "Cost after iteration 36, batch 0: 1.791738\n",
      "Cost after iteration 36, batch 10: 1.791745\n",
      "Cost after iteration 36, batch 20: 1.791740\n",
      "Cost after iteration 36, batch 30: 1.791798\n",
      "Cost after iteration 36: 1.791915\n",
      "Cost after iteration 37, batch 0: 1.791739\n",
      "Cost after iteration 37, batch 10: 1.791746\n",
      "Cost after iteration 37, batch 20: 1.791740\n",
      "Cost after iteration 37, batch 30: 1.791799\n",
      "Cost after iteration 37: 1.791916\n",
      "Cost after iteration 38, batch 0: 1.791739\n",
      "Cost after iteration 38, batch 10: 1.791746\n",
      "Cost after iteration 38, batch 20: 1.791740\n",
      "Cost after iteration 38, batch 30: 1.791799\n",
      "Cost after iteration 38: 1.791917\n",
      "Cost after iteration 39, batch 0: 1.791739\n",
      "Cost after iteration 39, batch 10: 1.791747\n",
      "Cost after iteration 39, batch 20: 1.791740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 39, batch 30: 1.791800\n",
      "Cost after iteration 39: 1.791919\n",
      "Cost after iteration 40, batch 0: 1.791740\n",
      "Cost after iteration 40, batch 10: 1.791747\n",
      "Cost after iteration 40, batch 20: 1.791741\n",
      "Cost after iteration 40, batch 30: 1.791801\n",
      "Cost after iteration 40: 1.791920\n",
      "Cost after iteration 41, batch 0: 1.791740\n",
      "Cost after iteration 41, batch 10: 1.791748\n",
      "Cost after iteration 41, batch 20: 1.791741\n",
      "Cost after iteration 41, batch 30: 1.791802\n",
      "Cost after iteration 41: 1.791921\n",
      "Cost after iteration 42, batch 0: 1.791741\n",
      "Cost after iteration 42, batch 10: 1.791748\n",
      "Cost after iteration 42, batch 20: 1.791741\n",
      "Cost after iteration 42, batch 30: 1.791803\n",
      "Cost after iteration 42: 1.791923\n",
      "Cost after iteration 43, batch 0: 1.791741\n",
      "Cost after iteration 43, batch 10: 1.791749\n",
      "Cost after iteration 43, batch 20: 1.791741\n",
      "Cost after iteration 43, batch 30: 1.791804\n",
      "Cost after iteration 43: 1.791924\n",
      "Cost after iteration 44, batch 0: 1.791741\n",
      "Cost after iteration 44, batch 10: 1.791749\n",
      "Cost after iteration 44, batch 20: 1.791742\n",
      "Cost after iteration 44, batch 30: 1.791805\n",
      "Cost after iteration 44: 1.791925\n",
      "Cost after iteration 45, batch 0: 1.791742\n",
      "Cost after iteration 45, batch 10: 1.791750\n",
      "Cost after iteration 45, batch 20: 1.791742\n",
      "Cost after iteration 45, batch 30: 1.791806\n",
      "Cost after iteration 45: 1.791927\n",
      "Cost after iteration 46, batch 0: 1.791742\n",
      "Cost after iteration 46, batch 10: 1.791751\n",
      "Cost after iteration 46, batch 20: 1.791742\n",
      "Cost after iteration 46, batch 30: 1.791806\n",
      "Cost after iteration 46: 1.791928\n",
      "Cost after iteration 47, batch 0: 1.791742\n",
      "Cost after iteration 47, batch 10: 1.791751\n",
      "Cost after iteration 47, batch 20: 1.791742\n",
      "Cost after iteration 47, batch 30: 1.791807\n",
      "Cost after iteration 47: 1.791929\n",
      "Cost after iteration 48, batch 0: 1.791743\n",
      "Cost after iteration 48, batch 10: 1.791752\n",
      "Cost after iteration 48, batch 20: 1.791743\n",
      "Cost after iteration 48, batch 30: 1.791808\n",
      "Cost after iteration 48: 1.791931\n",
      "Cost after iteration 49, batch 0: 1.791743\n",
      "Cost after iteration 49, batch 10: 1.791752\n",
      "Cost after iteration 49, batch 20: 1.791743\n",
      "Cost after iteration 49, batch 30: 1.791809\n",
      "Cost after iteration 49: 1.791932\n",
      "Cost after iteration 50, batch 0: 1.791744\n",
      "Cost after iteration 50, batch 10: 1.791753\n",
      "Cost after iteration 50, batch 20: 1.791743\n",
      "Cost after iteration 50, batch 30: 1.791810\n",
      "Cost after iteration 50: 1.791933\n",
      "Cost after iteration 51, batch 0: 1.791744\n",
      "Cost after iteration 51, batch 10: 1.791753\n",
      "Cost after iteration 51, batch 20: 1.791744\n",
      "Cost after iteration 51, batch 30: 1.791811\n",
      "Cost after iteration 51: 1.791935\n",
      "Cost after iteration 52, batch 0: 1.791744\n",
      "Cost after iteration 52, batch 10: 1.791754\n",
      "Cost after iteration 52, batch 20: 1.791744\n",
      "Cost after iteration 52, batch 30: 1.791812\n",
      "Cost after iteration 52: 1.791936\n",
      "Cost after iteration 53, batch 0: 1.791745\n",
      "Cost after iteration 53, batch 10: 1.791754\n",
      "Cost after iteration 53, batch 20: 1.791744\n",
      "Cost after iteration 53, batch 30: 1.791812\n",
      "Cost after iteration 53: 1.791937\n",
      "Cost after iteration 54, batch 0: 1.791745\n",
      "Cost after iteration 54, batch 10: 1.791755\n",
      "Cost after iteration 54, batch 20: 1.791744\n",
      "Cost after iteration 54, batch 30: 1.791813\n",
      "Cost after iteration 54: 1.791939\n",
      "Cost after iteration 55, batch 0: 1.791745\n",
      "Cost after iteration 55, batch 10: 1.791755\n",
      "Cost after iteration 55, batch 20: 1.791745\n",
      "Cost after iteration 55, batch 30: 1.791814\n",
      "Cost after iteration 55: 1.791940\n",
      "Cost after iteration 56, batch 0: 1.791746\n",
      "Cost after iteration 56, batch 10: 1.791756\n",
      "Cost after iteration 56, batch 20: 1.791745\n",
      "Cost after iteration 56, batch 30: 1.791815\n",
      "Cost after iteration 56: 1.791941\n",
      "Cost after iteration 57, batch 0: 1.791746\n",
      "Cost after iteration 57, batch 10: 1.791756\n",
      "Cost after iteration 57, batch 20: 1.791745\n",
      "Cost after iteration 57, batch 30: 1.791816\n",
      "Cost after iteration 57: 1.791943\n",
      "Cost after iteration 58, batch 0: 1.791746\n",
      "Cost after iteration 58, batch 10: 1.791757\n",
      "Cost after iteration 58, batch 20: 1.791745\n",
      "Cost after iteration 58, batch 30: 1.791817\n",
      "Cost after iteration 58: 1.791944\n",
      "Cost after iteration 59, batch 0: 1.791747\n",
      "Cost after iteration 59, batch 10: 1.791758\n",
      "Cost after iteration 59, batch 20: 1.791746\n",
      "Cost after iteration 59, batch 30: 1.791818\n",
      "Cost after iteration 59: 1.791945\n",
      "Cost after iteration 60, batch 0: 1.791747\n",
      "Cost after iteration 60, batch 10: 1.791758\n",
      "Cost after iteration 60, batch 20: 1.791746\n",
      "Cost after iteration 60, batch 30: 1.791818\n",
      "Cost after iteration 60: 1.791947\n",
      "Cost after iteration 61, batch 0: 1.791748\n",
      "Cost after iteration 61, batch 10: 1.791759\n",
      "Cost after iteration 61, batch 20: 1.791746\n",
      "Cost after iteration 61, batch 30: 1.791819\n",
      "Cost after iteration 61: 1.791948\n",
      "Cost after iteration 62, batch 0: 1.791748\n",
      "Cost after iteration 62, batch 10: 1.791759\n",
      "Cost after iteration 62, batch 20: 1.791746\n",
      "Cost after iteration 62, batch 30: 1.791820\n",
      "Cost after iteration 62: 1.791949\n",
      "Cost after iteration 63, batch 0: 1.791748\n",
      "Cost after iteration 63, batch 10: 1.791760\n",
      "Cost after iteration 63, batch 20: 1.791747\n",
      "Cost after iteration 63, batch 30: 1.791821\n",
      "Cost after iteration 63: 1.791951\n",
      "Cost after iteration 64, batch 0: 1.791749\n",
      "Cost after iteration 64, batch 10: 1.791760\n",
      "Cost after iteration 64, batch 20: 1.791747\n",
      "Cost after iteration 64, batch 30: 1.791822\n",
      "Cost after iteration 64: 1.791952\n",
      "Cost after iteration 65, batch 0: 1.791749\n",
      "Cost after iteration 65, batch 10: 1.791761\n",
      "Cost after iteration 65, batch 20: 1.791747\n",
      "Cost after iteration 65, batch 30: 1.791823\n",
      "Cost after iteration 65: 1.791953\n",
      "Cost after iteration 66, batch 0: 1.791749\n",
      "Cost after iteration 66, batch 10: 1.791761\n",
      "Cost after iteration 66, batch 20: 1.791748\n",
      "Cost after iteration 66, batch 30: 1.791824\n",
      "Cost after iteration 66: 1.791955\n",
      "Cost after iteration 67, batch 0: 1.791750\n",
      "Cost after iteration 67, batch 10: 1.791762\n",
      "Cost after iteration 67, batch 20: 1.791748\n",
      "Cost after iteration 67, batch 30: 1.791824\n",
      "Cost after iteration 67: 1.791956\n",
      "Cost after iteration 68, batch 0: 1.791750\n",
      "Cost after iteration 68, batch 10: 1.791762\n",
      "Cost after iteration 68, batch 20: 1.791748\n",
      "Cost after iteration 68, batch 30: 1.791825\n",
      "Cost after iteration 68: 1.791957\n",
      "Cost after iteration 69, batch 0: 1.791751\n",
      "Cost after iteration 69, batch 10: 1.791763\n",
      "Cost after iteration 69, batch 20: 1.791748\n",
      "Cost after iteration 69, batch 30: 1.791826\n",
      "Cost after iteration 69: 1.791959\n",
      "Cost after iteration 70, batch 0: 1.791751\n",
      "Cost after iteration 70, batch 10: 1.791763\n",
      "Cost after iteration 70, batch 20: 1.791749\n",
      "Cost after iteration 70, batch 30: 1.791827\n",
      "Cost after iteration 70: 1.791960\n",
      "Cost after iteration 71, batch 0: 1.791751\n",
      "Cost after iteration 71, batch 10: 1.791764\n",
      "Cost after iteration 71, batch 20: 1.791749\n",
      "Cost after iteration 71, batch 30: 1.791828\n",
      "Cost after iteration 71: 1.791961\n",
      "Cost after iteration 72, batch 0: 1.791752\n",
      "Cost after iteration 72, batch 10: 1.791764\n",
      "Cost after iteration 72, batch 20: 1.791749\n",
      "Cost after iteration 72, batch 30: 1.791829\n",
      "Cost after iteration 72: 1.791963\n",
      "Cost after iteration 73, batch 0: 1.791752\n",
      "Cost after iteration 73, batch 10: 1.791765\n",
      "Cost after iteration 73, batch 20: 1.791749\n",
      "Cost after iteration 73, batch 30: 1.791830\n",
      "Cost after iteration 73: 1.791964\n",
      "Cost after iteration 74, batch 0: 1.791752\n",
      "Cost after iteration 74, batch 10: 1.791765\n",
      "Cost after iteration 74, batch 20: 1.791750\n",
      "Cost after iteration 74, batch 30: 1.791831\n",
      "Cost after iteration 74: 1.791965\n",
      "Cost after iteration 75, batch 0: 1.791753\n",
      "Cost after iteration 75, batch 10: 1.791766\n",
      "Cost after iteration 75, batch 20: 1.791750\n",
      "Cost after iteration 75, batch 30: 1.791831\n",
      "Cost after iteration 75: 1.791967\n",
      "Cost after iteration 76, batch 0: 1.791753\n",
      "Cost after iteration 76, batch 10: 1.791767\n",
      "Cost after iteration 76, batch 20: 1.791750\n",
      "Cost after iteration 76, batch 30: 1.791832\n",
      "Cost after iteration 76: 1.791968\n",
      "Cost after iteration 77, batch 0: 1.791754\n",
      "Cost after iteration 77, batch 10: 1.791767\n",
      "Cost after iteration 77, batch 20: 1.791750\n",
      "Cost after iteration 77, batch 30: 1.791833\n",
      "Cost after iteration 77: 1.791969\n",
      "Cost after iteration 78, batch 0: 1.791754\n",
      "Cost after iteration 78, batch 10: 1.791768\n",
      "Cost after iteration 78, batch 20: 1.791751\n",
      "Cost after iteration 78, batch 30: 1.791834\n",
      "Cost after iteration 78: 1.791971\n",
      "Cost after iteration 79, batch 0: 1.791754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 79, batch 10: 1.791768\n",
      "Cost after iteration 79, batch 20: 1.791751\n",
      "Cost after iteration 79, batch 30: 1.791835\n",
      "Cost after iteration 79: 1.791972\n",
      "Cost after iteration 80, batch 0: 1.791755\n",
      "Cost after iteration 80, batch 10: 1.791769\n",
      "Cost after iteration 80, batch 20: 1.791751\n",
      "Cost after iteration 80, batch 30: 1.791836\n",
      "Cost after iteration 80: 1.791973\n",
      "Cost after iteration 81, batch 0: 1.791755\n",
      "Cost after iteration 81, batch 10: 1.791769\n",
      "Cost after iteration 81, batch 20: 1.791751\n",
      "Cost after iteration 81, batch 30: 1.791836\n",
      "Cost after iteration 81: 1.791974\n",
      "Cost after iteration 82, batch 0: 1.791755\n",
      "Cost after iteration 82, batch 10: 1.791770\n",
      "Cost after iteration 82, batch 20: 1.791752\n",
      "Cost after iteration 82, batch 30: 1.791837\n",
      "Cost after iteration 82: 1.791976\n",
      "Cost after iteration 83, batch 0: 1.791756\n",
      "Cost after iteration 83, batch 10: 1.791770\n",
      "Cost after iteration 83, batch 20: 1.791752\n",
      "Cost after iteration 83, batch 30: 1.791838\n",
      "Cost after iteration 83: 1.791977\n",
      "Cost after iteration 84, batch 0: 1.791756\n",
      "Cost after iteration 84, batch 10: 1.791771\n",
      "Cost after iteration 84, batch 20: 1.791752\n",
      "Cost after iteration 84, batch 30: 1.791839\n",
      "Cost after iteration 84: 1.791978\n",
      "Cost after iteration 85, batch 0: 1.791756\n",
      "Cost after iteration 85, batch 10: 1.791771\n",
      "Cost after iteration 85, batch 20: 1.791753\n",
      "Cost after iteration 85, batch 30: 1.791840\n",
      "Cost after iteration 85: 1.791980\n",
      "Cost after iteration 86, batch 0: 1.791757\n",
      "Cost after iteration 86, batch 10: 1.791772\n",
      "Cost after iteration 86, batch 20: 1.791753\n",
      "Cost after iteration 86, batch 30: 1.791841\n",
      "Cost after iteration 86: 1.791981\n",
      "Cost after iteration 87, batch 0: 1.791757\n",
      "Cost after iteration 87, batch 10: 1.791772\n",
      "Cost after iteration 87, batch 20: 1.791753\n",
      "Cost after iteration 87, batch 30: 1.791842\n",
      "Cost after iteration 87: 1.791982\n",
      "Cost after iteration 88, batch 0: 1.791758\n",
      "Cost after iteration 88, batch 10: 1.791773\n",
      "Cost after iteration 88, batch 20: 1.791753\n",
      "Cost after iteration 88, batch 30: 1.791842\n",
      "Cost after iteration 88: 1.791984\n",
      "Cost after iteration 89, batch 0: 1.791758\n",
      "Cost after iteration 89, batch 10: 1.791773\n",
      "Cost after iteration 89, batch 20: 1.791754\n",
      "Cost after iteration 89, batch 30: 1.791843\n",
      "Cost after iteration 89: 1.791985\n",
      "Cost after iteration 90, batch 0: 1.791758\n",
      "Cost after iteration 90, batch 10: 1.791774\n",
      "Cost after iteration 90, batch 20: 1.791754\n",
      "Cost after iteration 90, batch 30: 1.791844\n",
      "Cost after iteration 90: 1.791986\n",
      "Cost after iteration 91, batch 0: 1.791759\n",
      "Cost after iteration 91, batch 10: 1.791774\n",
      "Cost after iteration 91, batch 20: 1.791754\n",
      "Cost after iteration 91, batch 30: 1.791845\n",
      "Cost after iteration 91: 1.791988\n",
      "Cost after iteration 92, batch 0: 1.791759\n",
      "Cost after iteration 92, batch 10: 1.791775\n",
      "Cost after iteration 92, batch 20: 1.791754\n",
      "Cost after iteration 92, batch 30: 1.791846\n",
      "Cost after iteration 92: 1.791989\n",
      "Cost after iteration 93, batch 0: 1.791759\n",
      "Cost after iteration 93, batch 10: 1.791776\n",
      "Cost after iteration 93, batch 20: 1.791755\n",
      "Cost after iteration 93, batch 30: 1.791847\n",
      "Cost after iteration 93: 1.791990\n",
      "Cost after iteration 94, batch 0: 1.791760\n",
      "Cost after iteration 94, batch 10: 1.791776\n",
      "Cost after iteration 94, batch 20: 1.791755\n",
      "Cost after iteration 94, batch 30: 1.791848\n",
      "Cost after iteration 94: 1.791991\n",
      "Cost after iteration 95, batch 0: 1.791760\n",
      "Cost after iteration 95, batch 10: 1.791777\n",
      "Cost after iteration 95, batch 20: 1.791755\n",
      "Cost after iteration 95, batch 30: 1.791848\n",
      "Cost after iteration 95: 1.791993\n",
      "Cost after iteration 96, batch 0: 1.791761\n",
      "Cost after iteration 96, batch 10: 1.791777\n",
      "Cost after iteration 96, batch 20: 1.791755\n",
      "Cost after iteration 96, batch 30: 1.791849\n",
      "Cost after iteration 96: 1.791994\n",
      "Cost after iteration 97, batch 0: 1.791761\n",
      "Cost after iteration 97, batch 10: 1.791778\n",
      "Cost after iteration 97, batch 20: 1.791756\n",
      "Cost after iteration 97, batch 30: 1.791850\n",
      "Cost after iteration 97: 1.791995\n",
      "Cost after iteration 98, batch 0: 1.791761\n",
      "Cost after iteration 98, batch 10: 1.791778\n",
      "Cost after iteration 98, batch 20: 1.791756\n",
      "Cost after iteration 98, batch 30: 1.791851\n",
      "Cost after iteration 98: 1.791997\n",
      "Cost after iteration 99, batch 0: 1.791762\n",
      "Cost after iteration 99, batch 10: 1.791779\n",
      "Cost after iteration 99, batch 20: 1.791756\n",
      "Cost after iteration 99, batch 30: 1.791852\n",
      "Cost after iteration 99: 1.791998\n",
      "Cost after iteration 100, batch 0: 1.791762\n",
      "Cost after iteration 100, batch 10: 1.791779\n",
      "Cost after iteration 100, batch 20: 1.791756\n",
      "Cost after iteration 100, batch 30: 1.791853\n",
      "Cost after iteration 100: 1.791999\n",
      "Cost after iteration 101, batch 0: 1.791762\n",
      "Cost after iteration 101, batch 10: 1.791780\n",
      "Cost after iteration 101, batch 20: 1.791757\n",
      "Cost after iteration 101, batch 30: 1.791853\n",
      "Cost after iteration 101: 1.792001\n",
      "Cost after iteration 102, batch 0: 1.791763\n",
      "Cost after iteration 102, batch 10: 1.791780\n",
      "Cost after iteration 102, batch 20: 1.791757\n",
      "Cost after iteration 102, batch 30: 1.791854\n",
      "Cost after iteration 102: 1.792002\n",
      "Cost after iteration 103, batch 0: 1.791763\n",
      "Cost after iteration 103, batch 10: 1.791781\n",
      "Cost after iteration 103, batch 20: 1.791757\n",
      "Cost after iteration 103, batch 30: 1.791855\n",
      "Cost after iteration 103: 1.792003\n",
      "Cost after iteration 104, batch 0: 1.791763\n",
      "Cost after iteration 104, batch 10: 1.791781\n",
      "Cost after iteration 104, batch 20: 1.791757\n",
      "Cost after iteration 104, batch 30: 1.791856\n",
      "Cost after iteration 104: 1.792004\n",
      "Cost after iteration 105, batch 0: 1.791764\n",
      "Cost after iteration 105, batch 10: 1.791782\n",
      "Cost after iteration 105, batch 20: 1.791758\n",
      "Cost after iteration 105, batch 30: 1.791857\n",
      "Cost after iteration 105: 1.792006\n",
      "Cost after iteration 106, batch 0: 1.791764\n",
      "Cost after iteration 106, batch 10: 1.791782\n",
      "Cost after iteration 106, batch 20: 1.791758\n",
      "Cost after iteration 106, batch 30: 1.791858\n",
      "Cost after iteration 106: 1.792007\n",
      "Cost after iteration 107, batch 0: 1.791765\n",
      "Cost after iteration 107, batch 10: 1.791783\n",
      "Cost after iteration 107, batch 20: 1.791758\n",
      "Cost after iteration 107, batch 30: 1.791859\n",
      "Cost after iteration 107: 1.792008\n",
      "Cost after iteration 108, batch 0: 1.791765\n",
      "Cost after iteration 108, batch 10: 1.791783\n",
      "Cost after iteration 108, batch 20: 1.791759\n",
      "Cost after iteration 108, batch 30: 1.791859\n",
      "Cost after iteration 108: 1.792010\n",
      "Cost after iteration 109, batch 0: 1.791765\n",
      "Cost after iteration 109, batch 10: 1.791784\n",
      "Cost after iteration 109, batch 20: 1.791759\n",
      "Cost after iteration 109, batch 30: 1.791860\n",
      "Cost after iteration 109: 1.792011\n",
      "Cost after iteration 110, batch 0: 1.791766\n",
      "Cost after iteration 110, batch 10: 1.791784\n",
      "Cost after iteration 110, batch 20: 1.791759\n",
      "Cost after iteration 110, batch 30: 1.791861\n",
      "Cost after iteration 110: 1.792012\n",
      "Cost after iteration 111, batch 0: 1.791766\n",
      "Cost after iteration 111, batch 10: 1.791785\n",
      "Cost after iteration 111, batch 20: 1.791759\n",
      "Cost after iteration 111, batch 30: 1.791862\n",
      "Cost after iteration 111: 1.792013\n",
      "Cost after iteration 112, batch 0: 1.791766\n",
      "Cost after iteration 112, batch 10: 1.791785\n",
      "Cost after iteration 112, batch 20: 1.791760\n",
      "Cost after iteration 112, batch 30: 1.791863\n",
      "Cost after iteration 112: 1.792015\n",
      "Cost after iteration 113, batch 0: 1.791767\n",
      "Cost after iteration 113, batch 10: 1.791786\n",
      "Cost after iteration 113, batch 20: 1.791760\n",
      "Cost after iteration 113, batch 30: 1.791864\n",
      "Cost after iteration 113: 1.792016\n",
      "Cost after iteration 114, batch 0: 1.791767\n",
      "Cost after iteration 114, batch 10: 1.791786\n",
      "Cost after iteration 114, batch 20: 1.791760\n",
      "Cost after iteration 114, batch 30: 1.791864\n",
      "Cost after iteration 114: 1.792017\n",
      "Cost after iteration 115, batch 0: 1.791767\n",
      "Cost after iteration 115, batch 10: 1.791787\n",
      "Cost after iteration 115, batch 20: 1.791760\n",
      "Cost after iteration 115, batch 30: 1.791865\n",
      "Cost after iteration 115: 1.792019\n",
      "Cost after iteration 116, batch 0: 1.791768\n",
      "Cost after iteration 116, batch 10: 1.791787\n",
      "Cost after iteration 116, batch 20: 1.791761\n",
      "Cost after iteration 116, batch 30: 1.791866\n",
      "Cost after iteration 116: 1.792020\n",
      "Cost after iteration 117, batch 0: 1.791768\n",
      "Cost after iteration 117, batch 10: 1.791788\n",
      "Cost after iteration 117, batch 20: 1.791761\n",
      "Cost after iteration 117, batch 30: 1.791867\n",
      "Cost after iteration 117: 1.792021\n",
      "Cost after iteration 118, batch 0: 1.791768\n",
      "Cost after iteration 118, batch 10: 1.791789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 118, batch 20: 1.791761\n",
      "Cost after iteration 118, batch 30: 1.791868\n",
      "Cost after iteration 118: 1.792022\n",
      "Cost after iteration 119, batch 0: 1.791769\n",
      "Cost after iteration 119, batch 10: 1.791789\n",
      "Cost after iteration 119, batch 20: 1.791761\n",
      "Cost after iteration 119, batch 30: 1.791869\n",
      "Cost after iteration 119: 1.792024\n",
      "Cost after iteration 120, batch 0: 1.791769\n",
      "Cost after iteration 120, batch 10: 1.791790\n",
      "Cost after iteration 120, batch 20: 1.791762\n",
      "Cost after iteration 120, batch 30: 1.791869\n",
      "Cost after iteration 120: 1.792025\n",
      "Cost after iteration 121, batch 0: 1.791770\n",
      "Cost after iteration 121, batch 10: 1.791790\n",
      "Cost after iteration 121, batch 20: 1.791762\n",
      "Cost after iteration 121, batch 30: 1.791870\n",
      "Cost after iteration 121: 1.792026\n",
      "Cost after iteration 122, batch 0: 1.791770\n",
      "Cost after iteration 122, batch 10: 1.791791\n",
      "Cost after iteration 122, batch 20: 1.791762\n",
      "Cost after iteration 122, batch 30: 1.791871\n",
      "Cost after iteration 122: 1.792028\n",
      "Cost after iteration 123, batch 0: 1.791770\n",
      "Cost after iteration 123, batch 10: 1.791791\n",
      "Cost after iteration 123, batch 20: 1.791762\n",
      "Cost after iteration 123, batch 30: 1.791872\n",
      "Cost after iteration 123: 1.792029\n",
      "Cost after iteration 124, batch 0: 1.791771\n",
      "Cost after iteration 124, batch 10: 1.791792\n",
      "Cost after iteration 124, batch 20: 1.791763\n",
      "Cost after iteration 124, batch 30: 1.791873\n",
      "Cost after iteration 124: 1.792030\n",
      "Cost after iteration 125, batch 0: 1.791771\n",
      "Cost after iteration 125, batch 10: 1.791792\n",
      "Cost after iteration 125, batch 20: 1.791763\n",
      "Cost after iteration 125, batch 30: 1.791874\n",
      "Cost after iteration 125: 1.792031\n",
      "Cost after iteration 126, batch 0: 1.791771\n",
      "Cost after iteration 126, batch 10: 1.791793\n",
      "Cost after iteration 126, batch 20: 1.791763\n",
      "Cost after iteration 126, batch 30: 1.791875\n",
      "Cost after iteration 126: 1.792033\n",
      "Cost after iteration 127, batch 0: 1.791772\n",
      "Cost after iteration 127, batch 10: 1.791793\n",
      "Cost after iteration 127, batch 20: 1.791764\n",
      "Cost after iteration 127, batch 30: 1.791875\n",
      "Cost after iteration 127: 1.792034\n",
      "Cost after iteration 128, batch 0: 1.791772\n",
      "Cost after iteration 128, batch 10: 1.791794\n",
      "Cost after iteration 128, batch 20: 1.791764\n",
      "Cost after iteration 128, batch 30: 1.791876\n",
      "Cost after iteration 128: 1.792035\n",
      "Cost after iteration 129, batch 0: 1.791772\n",
      "Cost after iteration 129, batch 10: 1.791794\n",
      "Cost after iteration 129, batch 20: 1.791764\n",
      "Cost after iteration 129, batch 30: 1.791877\n",
      "Cost after iteration 129: 1.792037\n",
      "Cost after iteration 130, batch 0: 1.791773\n",
      "Cost after iteration 130, batch 10: 1.791795\n",
      "Cost after iteration 130, batch 20: 1.791764\n",
      "Cost after iteration 130, batch 30: 1.791878\n",
      "Cost after iteration 130: 1.792038\n",
      "Cost after iteration 131, batch 0: 1.791773\n",
      "Cost after iteration 131, batch 10: 1.791795\n",
      "Cost after iteration 131, batch 20: 1.791765\n",
      "Cost after iteration 131, batch 30: 1.791879\n",
      "Cost after iteration 131: 1.792039\n",
      "Cost after iteration 132, batch 0: 1.791774\n",
      "Cost after iteration 132, batch 10: 1.791796\n",
      "Cost after iteration 132, batch 20: 1.791765\n",
      "Cost after iteration 132, batch 30: 1.791880\n",
      "Cost after iteration 132: 1.792040\n",
      "Cost after iteration 133, batch 0: 1.791774\n",
      "Cost after iteration 133, batch 10: 1.791796\n",
      "Cost after iteration 133, batch 20: 1.791765\n",
      "Cost after iteration 133, batch 30: 1.791880\n",
      "Cost after iteration 133: 1.792042\n",
      "Cost after iteration 134, batch 0: 1.791774\n",
      "Cost after iteration 134, batch 10: 1.791797\n",
      "Cost after iteration 134, batch 20: 1.791765\n",
      "Cost after iteration 134, batch 30: 1.791881\n",
      "Cost after iteration 134: 1.792043\n",
      "Cost after iteration 135, batch 0: 1.791775\n",
      "Cost after iteration 135, batch 10: 1.791797\n",
      "Cost after iteration 135, batch 20: 1.791766\n",
      "Cost after iteration 135, batch 30: 1.791882\n",
      "Cost after iteration 135: 1.792044\n",
      "Cost after iteration 136, batch 0: 1.791775\n",
      "Cost after iteration 136, batch 10: 1.791798\n",
      "Cost after iteration 136, batch 20: 1.791766\n",
      "Cost after iteration 136, batch 30: 1.791883\n",
      "Cost after iteration 136: 1.792045\n",
      "Cost after iteration 137, batch 0: 1.791775\n",
      "Cost after iteration 137, batch 10: 1.791798\n",
      "Cost after iteration 137, batch 20: 1.791766\n",
      "Cost after iteration 137, batch 30: 1.791884\n",
      "Cost after iteration 137: 1.792047\n",
      "Cost after iteration 138, batch 0: 1.791776\n",
      "Cost after iteration 138, batch 10: 1.791799\n",
      "Cost after iteration 138, batch 20: 1.791766\n",
      "Cost after iteration 138, batch 30: 1.791885\n",
      "Cost after iteration 138: 1.792048\n",
      "Cost after iteration 139, batch 0: 1.791776\n",
      "Cost after iteration 139, batch 10: 1.791799\n",
      "Cost after iteration 139, batch 20: 1.791767\n",
      "Cost after iteration 139, batch 30: 1.791885\n",
      "Cost after iteration 139: 1.792049\n",
      "Cost after iteration 140, batch 0: 1.791776\n",
      "Cost after iteration 140, batch 10: 1.791800\n",
      "Cost after iteration 140, batch 20: 1.791767\n",
      "Cost after iteration 140, batch 30: 1.791886\n",
      "Cost after iteration 140: 1.792051\n",
      "Cost after iteration 141, batch 0: 1.791777\n",
      "Cost after iteration 141, batch 10: 1.791800\n",
      "Cost after iteration 141, batch 20: 1.791767\n",
      "Cost after iteration 141, batch 30: 1.791887\n",
      "Cost after iteration 141: 1.792052\n",
      "Cost after iteration 142, batch 0: 1.791777\n",
      "Cost after iteration 142, batch 10: 1.791801\n",
      "Cost after iteration 142, batch 20: 1.791767\n",
      "Cost after iteration 142, batch 30: 1.791888\n",
      "Cost after iteration 142: 1.792053\n",
      "Cost after iteration 143, batch 0: 1.791777\n",
      "Cost after iteration 143, batch 10: 1.791801\n",
      "Cost after iteration 143, batch 20: 1.791768\n",
      "Cost after iteration 143, batch 30: 1.791889\n",
      "Cost after iteration 143: 1.792054\n",
      "Cost after iteration 144, batch 0: 1.791778\n",
      "Cost after iteration 144, batch 10: 1.791802\n",
      "Cost after iteration 144, batch 20: 1.791768\n",
      "Cost after iteration 144, batch 30: 1.791890\n",
      "Cost after iteration 144: 1.792056\n",
      "Cost after iteration 145, batch 0: 1.791778\n",
      "Cost after iteration 145, batch 10: 1.791802\n",
      "Cost after iteration 145, batch 20: 1.791768\n",
      "Cost after iteration 145, batch 30: 1.791890\n",
      "Cost after iteration 145: 1.792057\n",
      "Cost after iteration 146, batch 0: 1.791779\n",
      "Cost after iteration 146, batch 10: 1.791803\n",
      "Cost after iteration 146, batch 20: 1.791768\n",
      "Cost after iteration 146, batch 30: 1.791891\n",
      "Cost after iteration 146: 1.792058\n",
      "Cost after iteration 147, batch 0: 1.791779\n",
      "Cost after iteration 147, batch 10: 1.791803\n",
      "Cost after iteration 147, batch 20: 1.791769\n",
      "Cost after iteration 147, batch 30: 1.791892\n",
      "Cost after iteration 147: 1.792059\n",
      "Cost after iteration 148, batch 0: 1.791779\n",
      "Cost after iteration 148, batch 10: 1.791804\n",
      "Cost after iteration 148, batch 20: 1.791769\n",
      "Cost after iteration 148, batch 30: 1.791893\n",
      "Cost after iteration 148: 1.792061\n",
      "Cost after iteration 149, batch 0: 1.791780\n",
      "Cost after iteration 149, batch 10: 1.791804\n",
      "Cost after iteration 149, batch 20: 1.791769\n",
      "Cost after iteration 149, batch 30: 1.791894\n",
      "Cost after iteration 149: 1.792062\n",
      "Cost after iteration 150, batch 0: 1.791780\n",
      "Cost after iteration 150, batch 10: 1.791805\n",
      "Cost after iteration 150, batch 20: 1.791769\n",
      "Cost after iteration 150, batch 30: 1.791894\n",
      "Cost after iteration 150: 1.792063\n",
      "Cost after iteration 151, batch 0: 1.791780\n",
      "Cost after iteration 151, batch 10: 1.791805\n",
      "Cost after iteration 151, batch 20: 1.791770\n",
      "Cost after iteration 151, batch 30: 1.791895\n",
      "Cost after iteration 151: 1.792064\n",
      "Cost after iteration 152, batch 0: 1.791781\n",
      "Cost after iteration 152, batch 10: 1.791806\n",
      "Cost after iteration 152, batch 20: 1.791770\n",
      "Cost after iteration 152, batch 30: 1.791896\n",
      "Cost after iteration 152: 1.792066\n",
      "Cost after iteration 153, batch 0: 1.791781\n",
      "Cost after iteration 153, batch 10: 1.791806\n",
      "Cost after iteration 153, batch 20: 1.791770\n",
      "Cost after iteration 153, batch 30: 1.791897\n",
      "Cost after iteration 153: 1.792067\n",
      "Cost after iteration 154, batch 0: 1.791781\n",
      "Cost after iteration 154, batch 10: 1.791807\n",
      "Cost after iteration 154, batch 20: 1.791770\n",
      "Cost after iteration 154, batch 30: 1.791898\n",
      "Cost after iteration 154: 1.792068\n",
      "Cost after iteration 155, batch 0: 1.791782\n",
      "Cost after iteration 155, batch 10: 1.791807\n",
      "Cost after iteration 155, batch 20: 1.791771\n",
      "Cost after iteration 155, batch 30: 1.791899\n",
      "Cost after iteration 155: 1.792070\n",
      "Cost after iteration 156, batch 0: 1.791782\n",
      "Cost after iteration 156, batch 10: 1.791808\n",
      "Cost after iteration 156, batch 20: 1.791771\n",
      "Cost after iteration 156, batch 30: 1.791899\n",
      "Cost after iteration 156: 1.792071\n",
      "Cost after iteration 157, batch 0: 1.791782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 157, batch 10: 1.791808\n",
      "Cost after iteration 157, batch 20: 1.791771\n",
      "Cost after iteration 157, batch 30: 1.791900\n",
      "Cost after iteration 157: 1.792072\n",
      "Cost after iteration 158, batch 0: 1.791783\n",
      "Cost after iteration 158, batch 10: 1.791809\n",
      "Cost after iteration 158, batch 20: 1.791771\n",
      "Cost after iteration 158, batch 30: 1.791901\n",
      "Cost after iteration 158: 1.792073\n",
      "Cost after iteration 159, batch 0: 1.791783\n",
      "Cost after iteration 159, batch 10: 1.791809\n",
      "Cost after iteration 159, batch 20: 1.791772\n",
      "Cost after iteration 159, batch 30: 1.791902\n",
      "Cost after iteration 159: 1.792075\n",
      "Cost after iteration 160, batch 0: 1.791783\n",
      "Cost after iteration 160, batch 10: 1.791810\n",
      "Cost after iteration 160, batch 20: 1.791772\n",
      "Cost after iteration 160, batch 30: 1.791903\n",
      "Cost after iteration 160: 1.792076\n",
      "Cost after iteration 161, batch 0: 1.791784\n",
      "Cost after iteration 161, batch 10: 1.791810\n",
      "Cost after iteration 161, batch 20: 1.791772\n",
      "Cost after iteration 161, batch 30: 1.791904\n",
      "Cost after iteration 161: 1.792077\n",
      "Cost after iteration 162, batch 0: 1.791784\n",
      "Cost after iteration 162, batch 10: 1.791811\n",
      "Cost after iteration 162, batch 20: 1.791772\n",
      "Cost after iteration 162, batch 30: 1.791904\n",
      "Cost after iteration 162: 1.792078\n",
      "Cost after iteration 163, batch 0: 1.791785\n",
      "Cost after iteration 163, batch 10: 1.791811\n",
      "Cost after iteration 163, batch 20: 1.791773\n",
      "Cost after iteration 163, batch 30: 1.791905\n",
      "Cost after iteration 163: 1.792080\n",
      "Cost after iteration 164, batch 0: 1.791785\n",
      "Cost after iteration 164, batch 10: 1.791812\n",
      "Cost after iteration 164, batch 20: 1.791773\n",
      "Cost after iteration 164, batch 30: 1.791906\n",
      "Cost after iteration 164: 1.792081\n",
      "Cost after iteration 165, batch 0: 1.791785\n",
      "Cost after iteration 165, batch 10: 1.791812\n",
      "Cost after iteration 165, batch 20: 1.791773\n",
      "Cost after iteration 165, batch 30: 1.791907\n",
      "Cost after iteration 165: 1.792082\n",
      "Cost after iteration 166, batch 0: 1.791786\n",
      "Cost after iteration 166, batch 10: 1.791813\n",
      "Cost after iteration 166, batch 20: 1.791774\n",
      "Cost after iteration 166, batch 30: 1.791908\n",
      "Cost after iteration 166: 1.792083\n",
      "Cost after iteration 167, batch 0: 1.791786\n",
      "Cost after iteration 167, batch 10: 1.791813\n",
      "Cost after iteration 167, batch 20: 1.791774\n",
      "Cost after iteration 167, batch 30: 1.791908\n",
      "Cost after iteration 167: 1.792085\n",
      "Cost after iteration 168, batch 0: 1.791786\n",
      "Cost after iteration 168, batch 10: 1.791814\n",
      "Cost after iteration 168, batch 20: 1.791774\n",
      "Cost after iteration 168, batch 30: 1.791909\n",
      "Cost after iteration 168: 1.792086\n",
      "Cost after iteration 169, batch 0: 1.791787\n",
      "Cost after iteration 169, batch 10: 1.791814\n",
      "Cost after iteration 169, batch 20: 1.791774\n",
      "Cost after iteration 169, batch 30: 1.791910\n",
      "Cost after iteration 169: 1.792087\n",
      "Cost after iteration 170, batch 0: 1.791787\n",
      "Cost after iteration 170, batch 10: 1.791815\n",
      "Cost after iteration 170, batch 20: 1.791775\n",
      "Cost after iteration 170, batch 30: 1.791911\n",
      "Cost after iteration 170: 1.792088\n",
      "Cost after iteration 171, batch 0: 1.791787\n",
      "Cost after iteration 171, batch 10: 1.791815\n",
      "Cost after iteration 171, batch 20: 1.791775\n",
      "Cost after iteration 171, batch 30: 1.791912\n",
      "Cost after iteration 171: 1.792090\n",
      "Cost after iteration 172, batch 0: 1.791788\n",
      "Cost after iteration 172, batch 10: 1.791816\n",
      "Cost after iteration 172, batch 20: 1.791775\n",
      "Cost after iteration 172, batch 30: 1.791912\n",
      "Cost after iteration 172: 1.792091\n",
      "Cost after iteration 173, batch 0: 1.791788\n",
      "Cost after iteration 173, batch 10: 1.791816\n",
      "Cost after iteration 173, batch 20: 1.791775\n",
      "Cost after iteration 173, batch 30: 1.791913\n",
      "Cost after iteration 173: 1.792092\n",
      "Cost after iteration 174, batch 0: 1.791788\n",
      "Cost after iteration 174, batch 10: 1.791817\n",
      "Cost after iteration 174, batch 20: 1.791776\n",
      "Cost after iteration 174, batch 30: 1.791914\n",
      "Cost after iteration 174: 1.792093\n",
      "Cost after iteration 175, batch 0: 1.791789\n",
      "Cost after iteration 175, batch 10: 1.791817\n",
      "Cost after iteration 175, batch 20: 1.791776\n",
      "Cost after iteration 175, batch 30: 1.791915\n",
      "Cost after iteration 175: 1.792095\n",
      "Cost after iteration 176, batch 0: 1.791789\n",
      "Cost after iteration 176, batch 10: 1.791818\n",
      "Cost after iteration 176, batch 20: 1.791776\n",
      "Cost after iteration 176, batch 30: 1.791916\n",
      "Cost after iteration 176: 1.792096\n",
      "Cost after iteration 177, batch 0: 1.791789\n",
      "Cost after iteration 177, batch 10: 1.791818\n",
      "Cost after iteration 177, batch 20: 1.791776\n",
      "Cost after iteration 177, batch 30: 1.791917\n",
      "Cost after iteration 177: 1.792097\n",
      "Cost after iteration 178, batch 0: 1.791790\n",
      "Cost after iteration 178, batch 10: 1.791819\n",
      "Cost after iteration 178, batch 20: 1.791777\n",
      "Cost after iteration 178, batch 30: 1.791917\n",
      "Cost after iteration 178: 1.792098\n",
      "Cost after iteration 179, batch 0: 1.791790\n",
      "Cost after iteration 179, batch 10: 1.791819\n",
      "Cost after iteration 179, batch 20: 1.791777\n",
      "Cost after iteration 179, batch 30: 1.791918\n",
      "Cost after iteration 179: 1.792100\n",
      "Cost after iteration 180, batch 0: 1.791790\n",
      "Cost after iteration 180, batch 10: 1.791820\n",
      "Cost after iteration 180, batch 20: 1.791777\n",
      "Cost after iteration 180, batch 30: 1.791919\n",
      "Cost after iteration 180: 1.792101\n",
      "Cost after iteration 181, batch 0: 1.791791\n",
      "Cost after iteration 181, batch 10: 1.791820\n",
      "Cost after iteration 181, batch 20: 1.791777\n",
      "Cost after iteration 181, batch 30: 1.791920\n",
      "Cost after iteration 181: 1.792102\n",
      "Cost after iteration 182, batch 0: 1.791791\n",
      "Cost after iteration 182, batch 10: 1.791821\n",
      "Cost after iteration 182, batch 20: 1.791778\n",
      "Cost after iteration 182, batch 30: 1.791921\n",
      "Cost after iteration 182: 1.792103\n",
      "Cost after iteration 183, batch 0: 1.791792\n",
      "Cost after iteration 183, batch 10: 1.791821\n",
      "Cost after iteration 183, batch 20: 1.791778\n",
      "Cost after iteration 183, batch 30: 1.791921\n",
      "Cost after iteration 183: 1.792105\n",
      "Cost after iteration 184, batch 0: 1.791792\n",
      "Cost after iteration 184, batch 10: 1.791822\n",
      "Cost after iteration 184, batch 20: 1.791778\n",
      "Cost after iteration 184, batch 30: 1.791922\n",
      "Cost after iteration 184: 1.792106\n",
      "Cost after iteration 185, batch 0: 1.791792\n",
      "Cost after iteration 185, batch 10: 1.791822\n",
      "Cost after iteration 185, batch 20: 1.791778\n",
      "Cost after iteration 185, batch 30: 1.791923\n",
      "Cost after iteration 185: 1.792107\n",
      "Cost after iteration 186, batch 0: 1.791793\n",
      "Cost after iteration 186, batch 10: 1.791823\n",
      "Cost after iteration 186, batch 20: 1.791779\n",
      "Cost after iteration 186, batch 30: 1.791924\n",
      "Cost after iteration 186: 1.792108\n",
      "Cost after iteration 187, batch 0: 1.791793\n",
      "Cost after iteration 187, batch 10: 1.791823\n",
      "Cost after iteration 187, batch 20: 1.791779\n",
      "Cost after iteration 187, batch 30: 1.791925\n",
      "Cost after iteration 187: 1.792109\n",
      "Cost after iteration 188, batch 0: 1.791793\n",
      "Cost after iteration 188, batch 10: 1.791824\n",
      "Cost after iteration 188, batch 20: 1.791779\n",
      "Cost after iteration 188, batch 30: 1.791925\n",
      "Cost after iteration 188: 1.792111\n",
      "Cost after iteration 189, batch 0: 1.791794\n",
      "Cost after iteration 189, batch 10: 1.791824\n",
      "Cost after iteration 189, batch 20: 1.791779\n",
      "Cost after iteration 189, batch 30: 1.791926\n",
      "Cost after iteration 189: 1.792112\n",
      "Cost after iteration 190, batch 0: 1.791794\n",
      "Cost after iteration 190, batch 10: 1.791825\n",
      "Cost after iteration 190, batch 20: 1.791780\n",
      "Cost after iteration 190, batch 30: 1.791927\n",
      "Cost after iteration 190: 1.792113\n",
      "Cost after iteration 191, batch 0: 1.791794\n",
      "Cost after iteration 191, batch 10: 1.791825\n",
      "Cost after iteration 191, batch 20: 1.791780\n",
      "Cost after iteration 191, batch 30: 1.791928\n",
      "Cost after iteration 191: 1.792114\n",
      "Cost after iteration 192, batch 0: 1.791795\n",
      "Cost after iteration 192, batch 10: 1.791826\n",
      "Cost after iteration 192, batch 20: 1.791780\n",
      "Cost after iteration 192, batch 30: 1.791929\n",
      "Cost after iteration 192: 1.792116\n",
      "Cost after iteration 193, batch 0: 1.791795\n",
      "Cost after iteration 193, batch 10: 1.791826\n",
      "Cost after iteration 193, batch 20: 1.791780\n",
      "Cost after iteration 193, batch 30: 1.791929\n",
      "Cost after iteration 193: 1.792117\n",
      "Cost after iteration 194, batch 0: 1.791795\n",
      "Cost after iteration 194, batch 10: 1.791827\n",
      "Cost after iteration 194, batch 20: 1.791781\n",
      "Cost after iteration 194, batch 30: 1.791930\n",
      "Cost after iteration 194: 1.792118\n",
      "Cost after iteration 195, batch 0: 1.791796\n",
      "Cost after iteration 195, batch 10: 1.791827\n",
      "Cost after iteration 195, batch 20: 1.791781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 195, batch 30: 1.791931\n",
      "Cost after iteration 195: 1.792119\n",
      "Cost after iteration 196, batch 0: 1.791796\n",
      "Cost after iteration 196, batch 10: 1.791828\n",
      "Cost after iteration 196, batch 20: 1.791781\n",
      "Cost after iteration 196, batch 30: 1.791932\n",
      "Cost after iteration 196: 1.792121\n",
      "Cost after iteration 197, batch 0: 1.791796\n",
      "Cost after iteration 197, batch 10: 1.791828\n",
      "Cost after iteration 197, batch 20: 1.791781\n",
      "Cost after iteration 197, batch 30: 1.791933\n",
      "Cost after iteration 197: 1.792122\n",
      "Cost after iteration 198, batch 0: 1.791797\n",
      "Cost after iteration 198, batch 10: 1.791829\n",
      "Cost after iteration 198, batch 20: 1.791782\n",
      "Cost after iteration 198, batch 30: 1.791933\n",
      "Cost after iteration 198: 1.792123\n",
      "Cost after iteration 199, batch 0: 1.791797\n",
      "Cost after iteration 199, batch 10: 1.791829\n",
      "Cost after iteration 199, batch 20: 1.791782\n",
      "Cost after iteration 199, batch 30: 1.791934\n",
      "Cost after iteration 199: 1.792124\n",
      "Cost after iteration 200, batch 0: 1.791797\n",
      "Cost after iteration 200, batch 10: 1.791830\n",
      "Cost after iteration 200, batch 20: 1.791782\n",
      "Cost after iteration 200, batch 30: 1.791935\n",
      "Cost after iteration 200: 1.792125\n",
      "Cost after iteration 201, batch 0: 1.791798\n",
      "Cost after iteration 201, batch 10: 1.791830\n",
      "Cost after iteration 201, batch 20: 1.791782\n",
      "Cost after iteration 201, batch 30: 1.791936\n",
      "Cost after iteration 201: 1.792127\n",
      "Cost after iteration 202, batch 0: 1.791798\n",
      "Cost after iteration 202, batch 10: 1.791831\n",
      "Cost after iteration 202, batch 20: 1.791783\n",
      "Cost after iteration 202, batch 30: 1.791937\n",
      "Cost after iteration 202: 1.792128\n",
      "Cost after iteration 203, batch 0: 1.791798\n",
      "Cost after iteration 203, batch 10: 1.791831\n",
      "Cost after iteration 203, batch 20: 1.791783\n",
      "Cost after iteration 203, batch 30: 1.791937\n",
      "Cost after iteration 203: 1.792129\n",
      "Cost after iteration 204, batch 0: 1.791799\n",
      "Cost after iteration 204, batch 10: 1.791832\n",
      "Cost after iteration 204, batch 20: 1.791783\n",
      "Cost after iteration 204, batch 30: 1.791938\n",
      "Cost after iteration 204: 1.792130\n",
      "Cost after iteration 205, batch 0: 1.791799\n",
      "Cost after iteration 205, batch 10: 1.791832\n",
      "Cost after iteration 205, batch 20: 1.791783\n",
      "Cost after iteration 205, batch 30: 1.791939\n",
      "Cost after iteration 205: 1.792132\n",
      "Cost after iteration 206, batch 0: 1.791799\n",
      "Cost after iteration 206, batch 10: 1.791833\n",
      "Cost after iteration 206, batch 20: 1.791784\n",
      "Cost after iteration 206, batch 30: 1.791940\n",
      "Cost after iteration 206: 1.792133\n",
      "Cost after iteration 207, batch 0: 1.791800\n",
      "Cost after iteration 207, batch 10: 1.791833\n",
      "Cost after iteration 207, batch 20: 1.791784\n",
      "Cost after iteration 207, batch 30: 1.791941\n",
      "Cost after iteration 207: 1.792134\n",
      "Cost after iteration 208, batch 0: 1.791800\n",
      "Cost after iteration 208, batch 10: 1.791834\n",
      "Cost after iteration 208, batch 20: 1.791784\n",
      "Cost after iteration 208, batch 30: 1.791941\n",
      "Cost after iteration 208: 1.792135\n",
      "Cost after iteration 209, batch 0: 1.791800\n",
      "Cost after iteration 209, batch 10: 1.791834\n",
      "Cost after iteration 209, batch 20: 1.791784\n",
      "Cost after iteration 209, batch 30: 1.791942\n",
      "Cost after iteration 209: 1.792137\n",
      "Cost after iteration 210, batch 0: 1.791801\n",
      "Cost after iteration 210, batch 10: 1.791835\n",
      "Cost after iteration 210, batch 20: 1.791784\n",
      "Cost after iteration 210, batch 30: 1.791943\n",
      "Cost after iteration 210: 1.792138\n",
      "Cost after iteration 211, batch 0: 1.791801\n",
      "Cost after iteration 211, batch 10: 1.791835\n",
      "Cost after iteration 211, batch 20: 1.791785\n",
      "Cost after iteration 211, batch 30: 1.791944\n",
      "Cost after iteration 211: 1.792139\n",
      "Cost after iteration 212, batch 0: 1.791801\n",
      "Cost after iteration 212, batch 10: 1.791836\n",
      "Cost after iteration 212, batch 20: 1.791785\n",
      "Cost after iteration 212, batch 30: 1.791945\n",
      "Cost after iteration 212: 1.792140\n",
      "Cost after iteration 213, batch 0: 1.791802\n",
      "Cost after iteration 213, batch 10: 1.791836\n",
      "Cost after iteration 213, batch 20: 1.791785\n",
      "Cost after iteration 213, batch 30: 1.791945\n",
      "Cost after iteration 213: 1.792141\n",
      "Cost after iteration 214, batch 0: 1.791802\n",
      "Cost after iteration 214, batch 10: 1.791837\n",
      "Cost after iteration 214, batch 20: 1.791785\n",
      "Cost after iteration 214, batch 30: 1.791946\n",
      "Cost after iteration 214: 1.792143\n",
      "Cost after iteration 215, batch 0: 1.791802\n",
      "Cost after iteration 215, batch 10: 1.791837\n",
      "Cost after iteration 215, batch 20: 1.791786\n",
      "Cost after iteration 215, batch 30: 1.791947\n",
      "Cost after iteration 215: 1.792144\n",
      "Cost after iteration 216, batch 0: 1.791803\n"
     ]
    }
   ],
   "source": [
    "# training model\n",
    "parameters = L_layer_model(train_data, train_labels, layers_dims, batch_size = 32, learning_rate = 0.0001, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 988
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 162655,
     "status": "ok",
     "timestamp": 1524424291554,
     "user": {
      "displayName": "Sam Shen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "100562431021972138930"
     },
     "user_tz": 420
    },
    "id": "XMFB3PxjKQ4t",
    "outputId": "e2ebca9c-8be3-409c-8db5-139e8ebca265"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0, batch 0: 0.037552\n",
      "Cost after iteration 0, batch 300: 0.018169\n",
      "Cost after iteration 0, batch 600: 0.021440\n",
      "Cost after iteration 0: 0.192616\n",
      "Cost after iteration 1, batch 0: 0.030991\n",
      "Cost after iteration 1, batch 300: 0.014042\n",
      "Cost after iteration 1, batch 600: 0.017460\n",
      "Cost after iteration 1: 0.178474\n",
      "Cost after iteration 2, batch 0: 0.026317\n",
      "Cost after iteration 2, batch 300: 0.011231\n",
      "Cost after iteration 2, batch 600: 0.015647\n",
      "Cost after iteration 2: 0.164508\n",
      "Cost after iteration 3, batch 0: 0.021732\n",
      "Cost after iteration 3, batch 300: 0.009307\n",
      "Cost after iteration 3, batch 600: 0.013995\n",
      "Cost after iteration 3: 0.151194\n",
      "Cost after iteration 4, batch 0: 0.019067\n",
      "Cost after iteration 4, batch 300: 0.007963\n",
      "Cost after iteration 4, batch 600: 0.012018\n",
      "Cost after iteration 4: 0.135523\n",
      "Cost after iteration 5, batch 0: 0.016594\n",
      "Cost after iteration 5, batch 300: 0.006841\n",
      "Cost after iteration 5, batch 600: 0.011045\n",
      "Cost after iteration 5: 0.120393\n",
      "Cost after iteration 6, batch 0: 0.014554\n",
      "Cost after iteration 6, batch 300: 0.005925\n",
      "Cost after iteration 6, batch 600: 0.009827\n",
      "Cost after iteration 6: 0.102484\n",
      "Cost after iteration 7, batch 0: 0.012761\n",
      "Cost after iteration 7, batch 300: 0.005329\n",
      "Cost after iteration 7, batch 600: 0.008771\n",
      "Cost after iteration 7: 0.085522\n",
      "Cost after iteration 8, batch 0: 0.011300\n",
      "Cost after iteration 8, batch 300: 0.004787\n",
      "Cost after iteration 8, batch 600: 0.007947\n",
      "Cost after iteration 8: 0.067754\n",
      "Cost after iteration 9, batch 0: 0.010012\n",
      "Cost after iteration 9, batch 300: 0.004277\n",
      "Cost after iteration 9, batch 600: 0.007009\n",
      "Cost after iteration 9: 0.052173\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVMAAAEVCAYAAABdSgYFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9ae6cf0470>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# further training on the same model \n",
    "parameters = L_layer_model(train_data, train_labels, layers_dims, parameters = parameters, learning_rate = 0.2, num_iterations = 10, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-f-nnqz-mfGo"
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XW8NLZ5_Plcc"
   },
   "outputs": [],
   "source": [
    "def eval_model(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((10,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(probas.shape[0]):\n",
    "        for j in range(probas.shape[1]):\n",
    "            if probas[i,j] == np.max(probas[:,j]):\n",
    "                p[i,j] = 1\n",
    "            else:\n",
    "                p[i,j] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(probas))\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    match = 0\n",
    "    for i in range(m):\n",
    "        match += int(np.array_equal(p[:,i],y[:,i]))\n",
    "    print(\"Accuracy: \"  + str(np.sum(match*1.0/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_vT032p2Pm0A"
   },
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(probas.shape[0]):\n",
    "        for j in range(probas.shape[1]):\n",
    "            if probas[i,j] == np.max(probas[:,j]):\n",
    "                p[0,j] = i\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(probas))\n",
    "    print (\"predictions: \" + str(int(np.squeeze(p))))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7999,
     "status": "ok",
     "timestamp": 1524424305787,
     "user": {
      "displayName": "Sam Shen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "100562431021972138930"
     },
     "user_tz": 420
    },
    "id": "ebpt464ANpnK",
    "outputId": "bd7bfa51-50bf-4ec5-c689-cf40424bcb8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model(train_data, train_labels, parameters)\n",
    "eval_model(eval_data, eval_labels, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 548,
     "status": "ok",
     "timestamp": 1524205134652,
     "user": {
      "displayName": "Sam Shen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "100562431021972138930"
     },
     "user_tz": 420
    },
    "id": "zGyOlEriv4lf",
    "outputId": "0af28b55-b550-4f3e-e2e2-f145ba7d74a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions: 5\n",
      "actual value: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe4ebb7f668>"
      ]
     },
     "execution_count": 82,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD4CAYAAADFJPs2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe4ebbfca90>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_num = 182\n",
    "predict(eval_data[:,image_num:image_num+1],parameters)\n",
    "print(\"actual value: \" + str(eval_labels_old[image_num]))\n",
    "data = mnist.test.images[image_num].reshape(28,28)\n",
    "plt.imshow(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Ew1O8bo7v4lg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "cnn_mnist.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
